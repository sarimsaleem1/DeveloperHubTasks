{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Task 2: Multi-Label Emotion Recognition\nfrom datasets import load_dataset\nfrom transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.metrics import f1_score, hamming_loss\nimport torch\nimport numpy as np\n\n# Step 1: Load Dataset\ndataset = load_dataset(\"go_emotions\")\nlabels = dataset['train'].features['labels'].feature.names\n\n# Step 2: Preprocessing\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\ndef tokenize(batch):\n    return tokenizer(batch['text'], padding=True, truncation=True)\n\ndataset = dataset.map(tokenize, batched=True)\ndataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n\n# Step 3: Model Definition\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(labels), problem_type=\"multi_label_classification\")\n\n# Step 4: Metrics\ndef compute_metrics(p):\n    preds = torch.sigmoid(torch.tensor(p.predictions)).numpy() > 0.5\n    labels = p.label_ids\n    return {\n        'f1': f1_score(labels, preds, average='micro'),\n        'hamming_loss': hamming_loss(labels, preds)\n    }\n\n# Step 5: Trainer\nargs = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2,\n    save_steps=500,\n    logging_steps=100,\n    save_total_limit=1,\n)\n\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=dataset['train'].shuffle(seed=42).select(range(1000)),  # small subset\n    eval_dataset=dataset['test'].select(range(200)),\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\nmetrics = trainer.evaluate()\nprint(metrics)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}